# BLIP CAM: Self Hosted Live Image Captioning with Real-Time/Offline Video ğŸ¥

This module provides image captioning using the BLIP (Bootstrapped Language-Image Pretraining) model. It supports real-time webcam streaming and offline video processing on Apple Silicon (M-series, MPS), with captions overlaid and basic performance metrics.

## ğŸš€ Features

- **Real-Time Video**: Webcam feed capture with overlaid captions
- **Offline Video**: Process existing video files, preview and optional MP4 export with captions
- **State-of-the-Art Captioning**: Uses `Salesforce/blip-image-captioning-large`
- **Apple Silicon Acceleration**: Runs on MPS (M-series)
- **Performance Monitoring**: Display FPS and accelerator status
- **Optimized Architecture**: Background thread for smooth streaming and caption generation

## ğŸ“‹ Requirements

- Python 3.8+
- Apple Silicon (M-series) recommended for MPS acceleration; CPU äº¦å¯è¿è¡Œ
- Webcamï¼ˆä»…å®æ—¶æ¨¡å¼éœ€è¦ï¼‰

### Core Dependencies
```
opencv-python>=4.5.0
torch>=1.9.0
transformers>=4.21.0
Pillow>=8.0.0
```

## ğŸ› ï¸ Installation

åœ¨æœ¬é¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰§è¡Œï¼š

```bash
cd BLIP_video
pip install -r requirements.txt
```

è¿è¡Œï¼ˆå®æ—¶æ‘„åƒå¤´ï¼‰ï¼š

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•
python BLIP_video/BLIP_CAM.py

# æˆ–åœ¨æ¨¡å—ç›®å½•
cd BLIP_video && python BLIP_CAM.py
```

ç¦»çº¿è§†é¢‘å¤„ç†ç¤ºä¾‹ï¼š

```bash
python BLIP_video/BLIP_CAM.py --video /path/to/input.mp4 --output /path/to/output.mp4
python BLIP_video/BLIP_CAM.py --video /path/to/input.mp4 --frame-interval 10 --no-display
```

ä½¿ç”¨æœ¬åœ°å·²ä¸‹è½½æ¨¡å‹ï¼ˆä¸è”ç½‘ä¸‹è½½ï¼‰ï¼š

```bash
python BLIP_video/BLIP_CAM.py --model-path /path/to/local/blip-image-captioning-large

# ç¦»çº¿è§†é¢‘ + æœ¬åœ°æ¨¡å‹
python BLIP_video/BLIP_CAM.py \
  --model-path /path/to/local/blip-image-captioning-large \
  --video /path/to/input.mp4 \
  --output /path/to/output.mp4
```

## ğŸ’¡ Use Cases

- **Accessibility Tools**: Real-time scene description for visually impaired users
- **Content Analysis**: Automated video content understanding and tagging
- **Smart Conferencing**: Enhanced video calls with automatic scene descriptions
- **Educational Tools**: Visual learning assistance and scene comprehension
- **Security Systems**: Intelligent surveillance with scene description capabilities

## ğŸ® Usage Controls

- Press `Q` to quit the application

## ğŸ”§ CLI Options

ç¨‹åºå‚æ•°ï¼š

- `--video`: ç¦»çº¿æ¨¡å¼è¾“å…¥è§†é¢‘è·¯å¾„ï¼ˆä¸ä¼ åˆ™å¼€å¯å®æ—¶æ‘„åƒå¤´ï¼‰
- `--output`: å¯é€‰ï¼Œå¯¼å‡ºå¸¦å­—å¹•çš„ MP4
- `--frame-interval`: ç¦»çº¿æ¨¡å¼ä¸‹æ¯ N å¸§é‡æ–°ç”Ÿæˆä¸€æ¬¡å­—å¹•ï¼ˆé»˜è®¤ 30ï¼‰
- `--no-display`: ä¸æ˜¾ç¤ºé¢„è§ˆçª—å£ï¼Œä»…å¤„ç†/å¯¼å‡º
- `--model-path`: æŒ‡å®šæœ¬åœ°æ¨¡å‹ç›®å½•ï¼ˆå·²ä¸‹è½½çš„ `Salesforce/blip-image-captioning-large`ï¼‰


## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Salesforce for the BLIP model
- PyTorch team for the deep learning framework
- Hugging Face for the transformers library
- å‚è€ƒé¡¹ç›®ï¼š[zawawiAI/BLIP_CAM](https://github.com/zawawiAI/BLIP_CAM.git)

## ğŸ“§ Contact

For questions and support, please open an issue in the GitHub repository or reach out to the maintainers.

---
â­ If you find this project useful, please consider giving it a star!

