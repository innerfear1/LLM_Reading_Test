{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a miniGPT from scratch\n",
    "### 1.import package\n",
    "### 2.Define GPT parameters\n",
    "### * 3.Define GPT structure\n",
    "    · 3.1 Single Head attention\n",
    "    · 3.2 Multi Head attention \n",
    "    · 3.3 Feed Forward(MLP)\n",
    "    · 3.4 block\n",
    "    · 3.5 GPT(embedding, position, norm, mlp, block)\n",
    "### * 4.Construct input Dataset\n",
    "### 5.run Function\n",
    "![GPT2](./pics/GPT2.png)\n",
    "### 1.import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "\n",
    "torch.manual_seed(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define GPT parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size : int = 512  # 文本最大长度\n",
    "    batch_size : int = 12\n",
    "    n_layer : int = 12\n",
    "    n_head : int = 12\n",
    "    n_embd : int = 768  # hidden_dim, hidden_size; 这里emb_size\n",
    "    hidden_dim : int = n_embd\n",
    "    # 为了可以ti_embedding_weight\n",
    "    dropout : float = 0.1\n",
    "    head_size : int = n_embd // n_head\n",
    "    # vocab_size, GPT2官方的tokenzier\n",
    "    vocab_size : int = 50274"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * 3. Define GPT structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    1.Single Head Attention\n",
    "'''\n",
    "class Single_Head_Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(config.hidden_dim, config.head_size)\n",
    "        self.value = nn.Linear(config.hidden_dim, config.head_size)\n",
    "        self.query = nn.Linear(config.hidden_dim, config.head_size)\n",
    "        # attention_mask 通过 register_buffer 注册\n",
    "        self.register_buffer(\n",
    "            'attention_mask',\n",
    "            # tril 下三角矩阵\n",
    "            torch.tril(\n",
    "                torch.ones(config.block_size, config.block_size)\n",
    "            )\n",
    "        )\n",
    "        self.dropout = nn.Droput(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, block_size, _ = x.size()\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        q = self.query(x)\n",
    "        weight = q @ k.transpose(-2, -1)\n",
    "        weight = weight.masked_fill(\n",
    "            self.attention_mask[:seq_len, :sqe_len] == 0,\n",
    "            float('-inf')\n",
    "        )\n",
    "        weight = F.softmax(weight, dim=-1) / math.sqrt(self.head_size)\n",
    "        weight = self.dropout(weight)\n",
    "        out = weight @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "'''\n",
    "    2.Multi Head Attention\n",
    "'''\n",
    "class Multi_Head_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads - nn.ModuleList(\n",
    "            [\n",
    "                Single_Head_Attention(config)\n",
    "                for _ in range(config.n_head)\n",
    "            ]\n",
    "        )\n",
    "        self.proj = nn.Liner(config.hidden_dim, config.hidden_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat(\n",
    "            [h(x) for h in self.heads], \n",
    "            dim=-1\n",
    "        )\n",
    "        output = self.proj(output)\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "'''\n",
    "    3.Feed Forward(MLP)\n",
    "'''\n",
    "class Feed_Forward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Liner(config.hidden_dim, 4 * config.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Liner(4 * config.hidden_dim, config.hidden_dim),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "'''\n",
    "    4.block\n",
    "'''\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.att = Multi_Head_Attention(config)\n",
    "        self.ffn = Feed_Forward(config)\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "'''\n",
    "    5.GPT\n",
    "    (embedding, position, norm, mlp, block)\n",
    "'''\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # embedding\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
    "        # block\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(config) for _ in range(config.n_layer)]\n",
    "        )\n",
    "        # norm\n",
    "        self.ln_final = nn.LayerNorm(config.n_embd)\n",
    "        # mlp\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # SLM 模型使用 tie_weight 减少参数\n",
    "        self.token_embedding_table.weight = self.lm_head.weight\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        batch_size, sqe_len = idx.size()\n",
    "        # embedding\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(sqe_len, device=idx.device))\n",
    "        x = token_emb + pos_emb\n",
    "        # block\n",
    "        x = self.blocks(x)\n",
    "        # norm\n",
    "        x = self.ln_final(x)\n",
    "        # mlp\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, sqe_len, vocab_size = logits.size()\n",
    "            logits = logits.view(batch_size * sqe_len, vocab_size)\n",
    "            targets = targets.view(batch_size * sqe_len)\n",
    "            \n",
    "        return logits, loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
