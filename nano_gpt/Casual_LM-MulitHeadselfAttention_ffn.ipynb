{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Decoder(Casual LM:self-attention+ffn)\n",
    "![Transformer Decoder][def]\n",
    "\n",
    "[def]: ./pics/Transformer_Decoder.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0767, 0.1227, 0.0396, 0.0906, 0.2155, 0.0697, 0.0790, 0.0774,\n",
       "          0.0394, 0.0785, 0.0360, 0.0748],\n",
       "         [0.0293, 0.0605, 0.0523, 0.0345, 0.0697, 0.0578, 0.0306, 0.2561,\n",
       "          0.2396, 0.0836, 0.0413, 0.0447],\n",
       "         [0.0643, 0.0681, 0.0486, 0.0475, 0.2576, 0.0567, 0.0422, 0.0796,\n",
       "          0.0195, 0.1085, 0.0461, 0.1613],\n",
       "         [0.1664, 0.1270, 0.0334, 0.0332, 0.1078, 0.0468, 0.0539, 0.1207,\n",
       "          0.1179, 0.0612, 0.0472, 0.0845]],\n",
       "\n",
       "        [[0.0976, 0.1298, 0.0698, 0.0789, 0.0697, 0.1016, 0.0583, 0.1136,\n",
       "          0.0700, 0.0611, 0.0662, 0.0835],\n",
       "         [0.1013, 0.0590, 0.1061, 0.0553, 0.1135, 0.0456, 0.0824, 0.1160,\n",
       "          0.0413, 0.1021, 0.0585, 0.1189],\n",
       "         [0.1001, 0.1262, 0.1524, 0.0966, 0.0322, 0.0819, 0.0294, 0.0571,\n",
       "          0.0670, 0.0922, 0.0446, 0.1201],\n",
       "         [0.2437, 0.0558, 0.0503, 0.0646, 0.1308, 0.0480, 0.0422, 0.0509,\n",
       "          0.0355, 0.1032, 0.0266, 0.1484]],\n",
       "\n",
       "        [[0.1289, 0.0274, 0.0966, 0.0739, 0.1011, 0.0564, 0.0679, 0.1046,\n",
       "          0.0342, 0.0451, 0.0471, 0.2167],\n",
       "         [0.0658, 0.1067, 0.0717, 0.0676, 0.0424, 0.0360, 0.0604, 0.0655,\n",
       "          0.0679, 0.0991, 0.2224, 0.0945],\n",
       "         [0.1433, 0.0239, 0.0751, 0.0965, 0.1581, 0.0558, 0.0777, 0.0640,\n",
       "          0.0374, 0.0430, 0.0583, 0.1668],\n",
       "         [0.1872, 0.0260, 0.0366, 0.0291, 0.0630, 0.1564, 0.2068, 0.0255,\n",
       "          0.0835, 0.0782, 0.0631, 0.0447]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleDecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, head_num, attention_dropout = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.head_dim = hidden_dim // head_num\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.att_dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "        self.att_ln = nn.LayerNorm(hidden_dim, eps = 1e-12)\n",
    "\n",
    "        # ffn(升维 -> 降维 -> att_ln)\n",
    "        self.up_proj = nn.Linear(hidden_dim, hidden_dim * 4)\n",
    "        self.down_proj = nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "\n",
    "        # 激活函数\n",
    "        self.act_fn = nn.GELU()\n",
    "\n",
    "        # dropout\n",
    "        self.ffn_dropout = nn.Dropout(attention_dropout)\n",
    "        self.ffn_ln = nn.LayerNorm(hidden_dim, eps = 1e-12)\n",
    "\n",
    "\n",
    "    def attention_Layer(self, query, key, value, attention_mask = None):\n",
    "        # output shape: [batch_size, seq_len, hidden_dim]\n",
    "        # Q, K, V shape: [batch_size, head_num, seq_len, head_dim]\n",
    "        key = key.transpose(2, 3)\n",
    "        attention_weight = query @ key / math.sqrt(self.head_dim)\n",
    "\n",
    "        # 自带的下三角矩阵以及attention_mask\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.tril()\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float(\"-1e20\")\n",
    "            )\n",
    "        else:\n",
    "            attention_mask = torch.ones_like(attention_weight).tril()\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float(\"-1e20\")\n",
    "            )\n",
    "        attention_weight =torch.softmax(attention_weight, dim = -1)\n",
    "        attention_weight = self.att_dropout(attention_weight)\n",
    "\n",
    "        out_mid = attention_weight @ value\n",
    "        # [batch_size, head_num, seq_len, head_dim] \n",
    "        #              -> [batch_size, seq_len, hidden_dim]\n",
    "        out_mid = out_mid.transpose(1, 2).contiguous()\n",
    "        batch_size, seq_len, _, _ = out_mid.size()\n",
    "        out_mid = out_mid.view(batch_size, seq_len, -1)\n",
    "\n",
    "        out = self.out_proj(out_mid)\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def MHA(self, x, attention_mask = None):\n",
    "        # x shape: [batch_size, seq_len, hidden_dim]\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Q, K, V shape: [batch_size, seq_len, hidden_dim]\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "\n",
    "        # [batch_size, seq_len, hidden_dim] -> [batch_size, head_num, seq_len, head_dim]\n",
    "        # hidden_dim -> head_num * head_dim\n",
    "        Q_state = Q.view(batch_size, seq_len, self.head_num, -1).transpose(1, 2)\n",
    "        K_state = K.view(batch_size, seq_len, self.head_num, -1).transpose(1, 2)\n",
    "        V_state = V.view(batch_size, seq_len, self.head_num, -1).transpose(1, 2)\n",
    "\n",
    "        output = self.attention_Layer(Q_state, K_state, V_state, attention_mask)    \n",
    "        # [batch_size, head_num, seq_len, head_dim] \n",
    "        #       -> [batch_size, seq_len, hidden_dim]\n",
    "        return self.att_ln(x + output)\n",
    "\n",
    "\n",
    "    def FFN(self, x):\n",
    "        # ffn(升维 -> 降维 -> att_ln)\n",
    "        up = self.up_proj(x)\n",
    "        act_up = self.act_fn(up)\n",
    "        down = self.down_proj(act_up)\n",
    "        drop_down = self.ffn_dropout(down)\n",
    "        # post LayerNorm\n",
    "        x = self.ffn_ln(x + drop_down)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask = None):\n",
    "        x = self.MHA(x, attention_mask)\n",
    "        x = self.FFN(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.layer_list = nn.ModuleList(\n",
    "            [\n",
    "                SimpleDecoderLayer(64, 8) for _ in range(5)\n",
    "            ]\n",
    "        )\n",
    "        self.emb = nn.Embedding(12, 64)\n",
    "        self.out = nn.Linear(64, 12)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # (b, s)\n",
    "        x = self.emb(x)\n",
    "        for i, l in enumerate(self.layer_list):\n",
    "            x = l(x, mask)\n",
    "        print(x.shape)\n",
    "        out = self.out(x)\n",
    "        return torch.softmax(out, dim = -1)\n",
    "\n",
    "# x = torch.rand(3, 4, 64)\n",
    "x = torch.randint(low=0, high=12, size=(3, 4))\n",
    "\n",
    "# net = SimpleDecoderLayer(64, 8)\n",
    "net = Decoder()\n",
    "\n",
    "mask = (\n",
    "    torch.tensor([[1, 0, 0, 0],\n",
    "                  [1, 1, 0, 0],\n",
    "                  [1, 1, 1, 0]]).unsqueeze(1).unsqueeze(2).repeat(1, 8, 4, 1)\n",
    ")\n",
    "net(x, mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
