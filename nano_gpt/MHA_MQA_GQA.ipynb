{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MHA -> MQA -> GQA\n",
    "![关系](./pics/MHA_MQA_GQA.png)\n",
    "### · coding Multi-Head-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.5567e-01,  2.4076e-01,  4.6895e-01, -3.6922e-02,  3.9975e-04,\n",
       "          -8.1978e-02, -3.2290e-02,  3.8538e-01,  4.8378e-01,  1.2396e-02,\n",
       "           4.5937e-02, -6.3421e-01,  4.0401e-01, -8.3526e-01,  1.4382e-01,\n",
       "          -2.6344e-01, -2.1243e-01, -7.8360e-02,  3.4908e-02, -1.7554e-01,\n",
       "           3.0448e-01, -1.0930e-02, -1.3162e-01, -1.8322e-01,  8.2782e-02,\n",
       "           8.8300e-01,  2.3892e-01,  1.4224e-01,  4.7802e-01,  4.7979e-01,\n",
       "          -7.9672e-01,  4.7388e-01,  5.0549e-01, -2.7657e-01, -5.8219e-01,\n",
       "           1.5372e-01, -6.4708e-01, -2.2582e-01,  4.8574e-01, -6.4262e-01,\n",
       "          -2.1122e-01,  7.3353e-01,  3.3868e-01, -2.2551e-02,  4.5843e-02,\n",
       "           7.1220e-01,  5.6761e-01, -1.4531e-01, -1.0196e+00,  3.3974e-01,\n",
       "           3.1525e-01, -9.0425e-01,  7.6110e-02,  6.1162e-01, -3.1718e-01,\n",
       "          -2.6062e-01, -6.2284e-01,  5.2902e-01,  3.9362e-01, -2.2538e-02,\n",
       "           1.9929e-01, -6.9513e-01,  2.3361e-02,  1.7679e-01, -6.6484e-01,\n",
       "          -5.3844e-01, -1.6639e-01, -2.6914e-01, -1.7933e-01, -1.7581e-01,\n",
       "          -1.0466e-01,  1.1801e-01,  1.9534e-01, -3.1665e-01,  1.2280e+00,\n",
       "           5.2437e-01,  2.1602e-01, -4.3006e-02,  2.8371e-01, -1.5107e-01,\n",
       "          -2.9274e-01, -2.4319e-01, -7.7992e-01, -1.9841e-02,  4.5357e-02,\n",
       "          -2.2027e-01,  7.8007e-02,  3.8582e-02, -6.7530e-01,  1.0493e-01,\n",
       "          -7.2009e-02, -4.5411e-01, -7.9896e-01,  6.2417e-02,  1.5523e-01,\n",
       "          -6.7660e-01, -1.0354e-01,  2.7250e-02, -4.6906e-01,  3.8068e-01,\n",
       "          -7.1615e-02,  1.5892e-01,  4.1512e-01, -1.1278e-01,  2.6334e-01,\n",
       "           2.0357e-01,  1.1571e-01, -7.2951e-02,  2.8084e-01, -8.1620e-01,\n",
       "           9.3037e-02,  3.7783e-01, -1.1085e-01,  3.7783e-01, -4.3623e-01,\n",
       "          -7.3831e-01, -4.2029e-01, -1.4933e-02, -3.5476e-01, -3.2945e-01,\n",
       "           2.8314e-01,  3.5014e-02, -5.5312e-01, -7.9365e-01, -1.0352e+00,\n",
       "          -2.6754e-01, -1.8449e-02, -3.8543e-01],\n",
       "         [-4.1526e-01,  3.0205e-01,  4.5247e-01, -5.0269e-02,  1.8553e-01,\n",
       "          -1.3517e-01,  2.3639e-02,  8.4310e-02,  2.7766e-01, -9.8681e-03,\n",
       "           6.4329e-02, -1.2982e-01,  4.4540e-01, -6.3630e-01,  1.7394e-01,\n",
       "          -3.4348e-01, -4.9779e-01, -1.0229e-01,  8.5566e-02, -1.3082e-01,\n",
       "           2.8713e-01, -1.1112e-01, -1.7660e-01, -3.5612e-01, -4.7644e-02,\n",
       "           4.6873e-01,  3.3594e-01, -1.2903e-01,  4.7520e-01,  4.2514e-01,\n",
       "          -6.5278e-01,  3.6666e-01,  5.4502e-01,  1.7652e-01, -4.5119e-01,\n",
       "           4.3865e-02, -4.0108e-01, -6.2534e-01,  3.5404e-01, -4.7374e-01,\n",
       "          -3.0424e-01,  6.0799e-01,  6.3058e-01, -1.6145e-01,  5.0763e-02,\n",
       "           6.7689e-01,  2.8661e-01,  2.0125e-01, -5.2951e-01,  4.0283e-01,\n",
       "           3.3558e-01, -5.5425e-01,  3.7548e-02,  4.8676e-01, -2.0758e-01,\n",
       "          -3.6229e-01, -6.2536e-01,  2.7717e-01,  1.7489e-01, -2.2317e-01,\n",
       "           1.3157e-01, -2.7252e-01,  3.5543e-01,  2.4512e-01, -5.1759e-01,\n",
       "          -3.2290e-01, -7.6342e-02,  6.7810e-02,  2.3525e-01, -5.2626e-01,\n",
       "           8.7279e-03, -6.8352e-02, -2.1981e-01, -3.4697e-01,  6.7230e-01,\n",
       "           2.5251e-01,  1.1832e-02, -1.7923e-01,  6.7387e-02, -4.2984e-02,\n",
       "          -1.1138e-02, -4.8917e-01, -7.8815e-01,  3.0049e-01,  3.0219e-01,\n",
       "          -8.8257e-02,  9.9394e-03,  2.5878e-02, -7.7395e-01,  1.0544e-01,\n",
       "          -1.8261e-01, -3.9282e-01, -3.3242e-01,  1.0185e-01,  2.4116e-01,\n",
       "          -6.4319e-01, -2.1371e-01, -5.6389e-02, -3.2714e-01,  3.5910e-01,\n",
       "           1.5145e-01,  9.5181e-02,  4.3983e-01, -2.3328e-01,  2.4155e-01,\n",
       "          -1.0165e-01,  2.1551e-02, -4.8880e-01,  2.7926e-01, -7.5381e-01,\n",
       "           1.3237e-01,  2.9064e-01,  2.3034e-01,  4.2830e-01, -2.1594e-03,\n",
       "          -5.6651e-01, -3.5806e-01,  3.6447e-01, -3.8030e-01, -2.2696e-01,\n",
       "           1.5097e-01, -3.6666e-01, -6.0718e-01, -9.4391e-01, -6.2233e-01,\n",
       "           4.3083e-02,  2.9085e-01, -4.5891e-01]],\n",
       "\n",
       "        [[ 1.2713e-01, -2.4297e-01,  7.4950e-02,  9.4066e-02, -1.9160e-02,\n",
       "          -1.6277e-01,  4.2553e-02, -2.8603e-03,  1.9427e-01,  3.4726e-01,\n",
       "          -2.2575e-01,  9.9541e-02, -1.9268e-01, -4.8791e-02,  1.6471e-02,\n",
       "           1.7291e-01, -1.5982e-01, -1.0526e-02, -2.4094e-01, -2.2798e-02,\n",
       "          -1.5016e-02, -8.7796e-02, -9.1140e-02, -6.3955e-01,  2.5354e-01,\n",
       "          -1.3622e-02,  8.3062e-02, -1.9197e-01, -5.9598e-02, -9.7627e-02,\n",
       "          -1.9656e-01,  4.4240e-02, -2.4530e-02,  1.7285e-01,  1.2382e-01,\n",
       "           2.4038e-01,  2.0921e-01, -1.4009e-01,  4.2290e-01,  3.4011e-01,\n",
       "           4.3537e-02, -7.7399e-02, -8.3207e-02,  5.0116e-02, -3.0283e-01,\n",
       "           4.2071e-02, -4.2510e-02,  1.4734e-02, -2.0889e-01, -1.9892e-01,\n",
       "          -6.2604e-03,  2.3739e-02, -1.1043e-01,  1.2303e-01, -2.1154e-01,\n",
       "          -2.5969e-01,  3.5557e-02,  1.5941e-01, -3.0421e-02,  1.6842e-01,\n",
       "          -5.9768e-02,  1.7704e-01, -3.5665e-02,  2.4124e-02, -1.5968e-01,\n",
       "          -1.8377e-01, -4.3788e-02, -2.6790e-01, -1.0893e-01, -2.7755e-01,\n",
       "          -1.9402e-01,  4.0097e-01,  1.5806e-01, -8.5546e-02, -2.4585e-01,\n",
       "           2.5744e-01,  2.6958e-01, -2.0013e-01,  1.2181e-01, -3.8451e-01,\n",
       "           2.2853e-01,  1.8199e-02, -2.6221e-01,  1.4549e-01, -4.1447e-01,\n",
       "           4.9118e-01,  1.0415e-01,  1.4720e-01,  1.0544e-01, -8.7546e-02,\n",
       "           3.0821e-01,  2.7987e-01, -2.3852e-02,  2.0128e-01, -4.2133e-01,\n",
       "           2.6164e-01, -1.8798e-01,  1.2883e-02, -5.8510e-01,  1.4947e-01,\n",
       "           5.9426e-02, -2.0174e-01,  2.5992e-02, -2.2213e-01,  5.7266e-01,\n",
       "          -1.6405e-01,  1.4876e-01,  2.3776e-01,  1.1410e-01,  1.5346e-01,\n",
       "          -6.9385e-02,  1.2427e-01,  5.0533e-02,  9.3425e-02,  1.1756e-01,\n",
       "          -7.2141e-02, -2.5197e-01, -1.4959e-01,  2.6473e-02, -4.9468e-02,\n",
       "          -2.5979e-01, -2.5035e-01, -1.6027e-02,  1.3629e-01,  9.1798e-02,\n",
       "           8.7069e-02,  2.5611e-01,  1.8778e-01],\n",
       "         [ 9.1519e-02, -1.4699e-01, -2.4688e-02, -3.1519e-04, -6.1010e-02,\n",
       "          -1.3569e-01,  2.3892e-02,  9.6081e-02,  1.2219e-01,  3.8811e-01,\n",
       "          -2.4737e-01,  1.5089e-01, -8.7001e-02, -1.2942e-02,  4.6032e-02,\n",
       "           5.7961e-02, -1.0181e-01, -2.1668e-01, -6.0450e-02, -2.2879e-01,\n",
       "          -2.9195e-02, -1.7294e-01,  8.6146e-02, -5.5888e-01,  2.4801e-01,\n",
       "          -4.2046e-02, -4.8831e-02, -7.8720e-02, -1.0556e-01, -1.1985e-01,\n",
       "          -2.5088e-01, -1.0382e-02,  1.4348e-01,  1.2137e-01, -5.2747e-03,\n",
       "           2.7943e-01,  2.2952e-01, -1.9211e-01,  3.6109e-01,  2.3875e-01,\n",
       "           1.6107e-01, -1.7661e-01, -3.0754e-02,  1.0213e-01, -2.3720e-01,\n",
       "           6.6889e-02, -1.5591e-02,  3.1551e-03, -1.9143e-01, -2.7660e-01,\n",
       "          -2.2136e-02,  1.3736e-01, -1.9469e-01,  6.9059e-02, -2.8378e-01,\n",
       "          -1.5672e-01, -2.9978e-02,  1.0234e-01, -1.0020e-01,  1.5942e-01,\n",
       "          -7.5285e-02,  1.7982e-01,  6.5587e-02,  8.7201e-02, -2.0960e-01,\n",
       "          -1.0222e-01, -4.0063e-02, -2.3974e-01, -1.5556e-01, -1.3710e-01,\n",
       "          -2.1616e-01,  3.3607e-01,  1.6836e-01, -5.2896e-03, -1.9722e-02,\n",
       "           1.7855e-01,  1.8379e-01, -2.2316e-01,  9.9481e-02, -4.2640e-01,\n",
       "           1.0456e-01, -2.1073e-01, -2.5435e-01,  1.2585e-01, -3.8889e-01,\n",
       "           2.4959e-01,  1.0001e-01,  1.8702e-01,  5.7086e-02, -3.2142e-01,\n",
       "           3.3665e-01,  2.3357e-01,  7.3965e-02,  2.1583e-02, -4.5037e-01,\n",
       "           3.0751e-01, -1.3670e-01,  1.1656e-01, -6.1898e-01,  3.4813e-01,\n",
       "           2.0962e-01, -2.7116e-01, -7.3271e-03, -2.1092e-01,  5.7112e-01,\n",
       "           7.8062e-02,  1.3974e-01,  2.0564e-01,  8.0534e-02,  3.6441e-01,\n",
       "          -1.0134e-02,  1.1814e-01,  9.2086e-03,  2.6046e-01,  2.4040e-01,\n",
       "          -6.1274e-02, -9.1295e-02, -1.0383e-01,  1.0124e-02,  1.0989e-01,\n",
       "          -1.6165e-01, -2.7640e-01, -8.9698e-02,  1.7197e-01,  2.8447e-01,\n",
       "           1.7671e-01,  1.6593e-01, -4.8830e-02]],\n",
       "\n",
       "        [[ 1.0102e-01, -6.6475e-02, -3.0341e-01, -4.0580e-01, -2.7608e-01,\n",
       "           3.1605e-01,  3.2279e-01,  2.2180e-01, -2.7125e-01, -7.4762e-02,\n",
       "          -3.6691e-01, -5.4933e-01, -7.5664e-01,  4.3474e-01,  3.3358e-03,\n",
       "           1.0432e-01, -2.4442e-01, -2.5923e-01,  4.5206e-01, -2.2852e-01,\n",
       "          -6.8282e-02,  4.2476e-01,  3.6207e-01, -1.9024e-01,  1.9092e-01,\n",
       "          -5.6174e-02,  5.9686e-01, -5.1541e-01, -3.2071e-01,  1.8345e-01,\n",
       "          -1.8711e-01, -6.7862e-01,  2.2165e-01,  4.1789e-01,  2.1069e-01,\n",
       "           4.3762e-01, -2.5454e-01,  7.9249e-01,  2.9898e-01, -6.9852e-02,\n",
       "           2.5013e-01,  1.3212e-01,  1.7726e-01,  2.3921e-01,  3.4056e-01,\n",
       "           5.9824e-01,  7.8298e-02, -7.6296e-02,  2.1130e-01, -2.3948e-01,\n",
       "           2.6200e-01, -7.7615e-02,  1.0121e-01, -2.5287e-01,  1.7560e-01,\n",
       "           4.7945e-01,  4.4995e-01, -4.0718e-01,  1.5669e-02,  2.0458e-01,\n",
       "           1.4968e-01, -4.8873e-01,  2.3086e-01,  3.2403e-02,  1.7436e-01,\n",
       "          -1.0010e-01, -1.1369e-01, -2.4122e-01, -2.4364e-01,  1.5778e-01,\n",
       "           7.4241e-01,  3.1236e-02, -9.7725e-02,  1.1238e-01,  3.1938e-01,\n",
       "           3.0583e-01, -3.8834e-01, -4.3980e-02,  2.4228e-01,  2.6260e-01,\n",
       "           4.7329e-01,  6.3853e-01,  3.4916e-01,  1.3624e-01,  1.0668e-01,\n",
       "          -1.1137e-01, -9.2889e-02,  4.6028e-01,  3.6242e-01, -8.1062e-01,\n",
       "           1.3664e-01, -1.8380e-01,  5.2457e-01, -5.5129e-02,  2.4600e-01,\n",
       "          -5.6793e-01,  1.8485e-01,  5.3949e-01,  2.5851e-01,  4.6645e-01,\n",
       "          -3.9040e-01,  4.0829e-02, -3.8898e-01, -1.9002e-02,  3.4419e-01,\n",
       "           6.1957e-01, -1.6840e-01, -1.4369e-01,  4.0928e-01,  9.1028e-01,\n",
       "           9.0001e-02,  7.0680e-03, -4.5502e-02,  5.9618e-02,  4.6204e-01,\n",
       "          -4.6527e-02,  3.0330e-01, -3.2145e-01, -5.7771e-03,  4.2148e-01,\n",
       "           5.7339e-01,  5.3510e-01, -2.1541e-01, -2.2336e-01,  3.5043e-01,\n",
       "           1.1322e-01,  8.5360e-02, -2.2327e-01],\n",
       "         [ 4.5464e-02, -3.1695e-02, -2.2326e-01, -4.9651e-01, -2.3217e-01,\n",
       "           1.4846e-01,  1.5831e-01,  1.3991e-01,  3.6884e-03, -5.6111e-02,\n",
       "          -1.8988e-01, -5.4953e-01, -6.7438e-01,  4.1199e-01,  2.3525e-01,\n",
       "           1.7297e-01, -3.3067e-01, -2.4164e-01,  5.1499e-01, -2.7696e-01,\n",
       "          -5.8356e-02,  4.5441e-01,  6.2907e-01, -2.1836e-01,  4.8242e-03,\n",
       "           1.9062e-01,  4.5481e-01, -4.1932e-01, -1.0908e-01,  2.1161e-02,\n",
       "          -4.2031e-01, -6.5556e-01,  2.3908e-01,  4.0355e-01,  8.2250e-02,\n",
       "           4.3183e-01, -2.8067e-01,  6.0528e-01,  2.0992e-01, -1.4105e-01,\n",
       "          -6.3011e-02,  3.0622e-01,  1.1578e-01,  2.5353e-01,  4.8293e-01,\n",
       "           5.7546e-01,  7.9729e-02, -1.1178e-01, -1.0270e-02, -1.8876e-01,\n",
       "           3.7150e-01, -1.5221e-01,  1.0605e-01, -2.8810e-01,  2.1157e-01,\n",
       "           3.6874e-01,  3.4576e-01, -3.8129e-01,  1.2141e-01,  2.2298e-01,\n",
       "           1.8466e-01, -7.2148e-01,  1.1254e-01,  5.0910e-02,  1.1399e-01,\n",
       "          -2.7052e-01, -2.3103e-01, -5.0771e-01, -2.2571e-01,  1.7215e-01,\n",
       "           7.8763e-01, -4.2821e-02,  2.6778e-02, -1.4011e-02,  3.3446e-01,\n",
       "           1.6938e-01, -4.8418e-01,  1.5114e-01,  1.4250e-01,  1.5865e-01,\n",
       "           2.7189e-01,  6.4541e-01,  7.2730e-02,  1.2113e-04,  1.9369e-01,\n",
       "          -2.6200e-01, -6.3393e-02,  3.2313e-01,  1.9852e-01, -7.0163e-01,\n",
       "           2.0708e-02, -6.8261e-02,  2.0510e-01, -1.1949e-01,  1.6458e-01,\n",
       "          -5.4487e-01,  1.9256e-01,  3.5322e-01,  1.9090e-01,  3.9889e-01,\n",
       "          -3.6918e-01,  8.1623e-02, -2.3139e-01, -4.7687e-02,  1.7450e-01,\n",
       "           5.0694e-01, -5.4722e-02, -9.6234e-02,  2.1274e-01,  6.3577e-01,\n",
       "          -1.8081e-01,  1.5845e-01, -2.3902e-01, -4.7623e-03,  4.2043e-01,\n",
       "          -1.5560e-01,  2.1918e-01, -2.7737e-01,  1.9261e-02,  2.0767e-01,\n",
       "           5.6605e-01,  5.6211e-01, -1.8442e-01, -1.3181e-01,  2.3195e-01,\n",
       "          -1.1642e-01,  2.1724e-01, -1.0065e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, head_num, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.head_dim = hidden_dim // head_num\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.att_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, attention_mask = None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        Q_state = q.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        K_state = k.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        V_state = v.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attention_weight = Q_state @ K_state.transpose(-1, -2) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float(\"-inf\")\n",
    "            )\n",
    "        \n",
    "        attention_weight = F.softmax(attention_weight, dim = -1)\n",
    "        attention_weight = self.att_dropout(attention_weight)\n",
    "\n",
    "        output = attention_weight @ V_state\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "        \n",
    "# Test\n",
    "attention_mask_MHA = (\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [0, 1],\n",
    "            [1, 1],\n",
    "            [1, 0]\n",
    "        ]\n",
    "    ).unsqueeze(1).unsqueeze(2).expand(3, 8, 2, 2)\n",
    ")\n",
    "\n",
    "x_MHA = torch.randn(3, 2, 128)\n",
    "\n",
    "# head_dim = 16\n",
    "MHA_net = MultiHeadAttention(128, 8)\n",
    "MHA_net(x_MHA, attention_mask_MHA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Query Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, head_num, num_key_value_head = 1, attention_dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.num_key_value_head = num_key_value_head\n",
    "        self.head_dim = hidden_dim // head_num\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_dim, head_num * self.head_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, num_key_value_head * self.head_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, num_key_value_head * self.head_dim)\n",
    "        self.out_proj = nn. Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, x, attention_mask = None):\n",
    "        # x shape:[b,s,h]\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "\n",
    "        # Q_state shape [b, s, head_num, head_dim]\n",
    "        Q_state = Q.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        K_state = K.view(batch_size, seq_len, self.num_key_value_head, self.head_dim).transpose(1, 2)\n",
    "        V_state = V.view(batch_size, seq_len, self.num_key_value_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # K, V repeat_interleave\n",
    "        K_state = K_state.repeat_interleave(self.head_num // self.num_key_value_head, dim = 1)\n",
    "        V_state = V_state.repeat_interleave(self.head_num // self.num_key_value_head, dim = 1)\n",
    "\n",
    "        # attention_weight shape [b, head_num, seq_len, seq_len]\n",
    "        attenention_weight = Q_state @ K_state.transpose(-1, -2) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attenention_weight = attenention_weight.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float(\"-inf\")\n",
    "            )\n",
    "\n",
    "        attenention_weight = F.softmax(attenention_weight, dim = -1)\n",
    "        output_mid = self.attention_dropout(attenention_weight) @ V_state\n",
    "\n",
    "        # output shape: [batch_size, head_num, seq_len, head_dim] \n",
    "        #                   -> [batch_size, seq_len, head_num * head_dim]\n",
    "        output = output_mid.transpose(1, 2).contiguous().view(batch_size, seq_len, -1) \n",
    "\n",
    "        output = self.out_proj(output)       \n",
    "        return output\n",
    "\n",
    "# Test\n",
    "x_MQA = torch.randn(3, 2, 128)\n",
    "attention_mask_MQA = (\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [0, 1],\n",
    "            [1, 1],\n",
    "            [1, 0]\n",
    "        ]\n",
    "    ).unsqueeze(1).unsqueeze(2).expand(3, 8, 2, 2)\n",
    ")\n",
    "# head_dim = 16\n",
    "MQA_net = MultiQueryAttention(128, 8)\n",
    "MQA_net(x_GQA, attention_mask_MQA).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Query Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 128])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, head_num, num_key_value_head, attention_dropout = 0.1):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % head_num == 0, \"hidden_dim must be divisible by head_num\"\n",
    "        assert head_num % num_key_value_head == 0, \"head_num must be divisible by num_key_value_head\"\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        # head number * head_dim = hidden_dim\n",
    "        self.head_dim = hidden_dim // head_num\n",
    "        self.num_key_value_head = num_key_value_head\n",
    "\n",
    "        # K,V out features = head_num * head_dim\n",
    "        self.q_proj = nn.Linear(hidden_dim, head_num * self.head_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, num_key_value_head * self.head_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, num_key_value_head * self.head_dim)\n",
    "        # Q out features = head_num * head_dim = hidden_dim\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim) \n",
    "\n",
    "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, x, attention_mask = None):\n",
    "        # x shape: [batch_size, seq_len, hidden_dim]\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        Q = self.q_proj(x)  # [batch_size, seq_len, head_num * head_dim]\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "\n",
    "        # [batch_size, seq_len, head_num * head_dim] -> [batch_size, seq_len, head_num, head_dim]\n",
    "        # head_num * head_dim -> head_num * head_dim\n",
    "        Q_state = Q.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        K_state = K.view(batch_size, seq_len, self.num_key_value_head, self.head_dim).transpose(1, 2)\n",
    "        V_state = V.view(batch_size, seq_len, self.num_key_value_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # head_num 和 num_key_value_head 可能本相同，因此K,V需要repeat操作\n",
    "        K_state = K_state.repeat_interleave(self.head_num // self.num_key_value_head, dim = 1)\n",
    "        V_state = V_state.repeat_interleave(self.head_num // self.num_key_value_head, dim = 1)\n",
    "\n",
    "        # attention_weight 目标shape -> [batch_size, head_num, seq_len, seq_len]\n",
    "        attention_weight = Q_state @ K_state.transpose(2, 3) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # attention_mask shape: [batch_size, seq_len, seq_len]\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0, \n",
    "                float(\"-inf\")\n",
    "            )\n",
    "\n",
    "        attention_weight = torch.softmax(attention_weight, dim = -1)\n",
    "        output = self.attention_dropout(attention_weight) @ V_state\n",
    "\n",
    "        # output shape: [batch_size, head_num, seq_len, head_dim] \n",
    "        #                   -> [batch_size, seq_len, head_num * head_dim]\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "\n",
    "        output = self.out_proj(output)\n",
    "        return output\n",
    "\n",
    "# Test\n",
    "x_GQA = torch.randn(3, 2, 128)\n",
    "attention_mask_GQA = (\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [0, 1],\n",
    "            [1, 1],\n",
    "            [1, 0]\n",
    "        ]\n",
    "    ).unsqueeze(1).unsqueeze(2).expand(3, 8, 2, 2)\n",
    ")\n",
    "# head_dim = 16\n",
    "GQA_net = GroupQueryAttention(128, 8, 4)\n",
    "GQA_net(x_GQA, attention_mask_GQA).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
