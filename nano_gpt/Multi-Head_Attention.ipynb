{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "![Multi-Head Self Attention](./pics/Multi-head_attention.png)\n",
    "### ·公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0993e-01,  3.0547e-01, -2.3037e-05,  6.8221e-01, -1.2496e-02,\n",
       "           2.5840e-01,  2.3654e-01,  1.3889e-01, -2.1414e-01,  4.3203e-01,\n",
       "           3.9672e-01, -2.6677e-02, -2.9180e-01,  5.1848e-02, -2.3070e-01,\n",
       "           9.0426e-02,  5.0347e-01,  3.0282e-01, -3.1114e-01,  2.1110e-02,\n",
       "          -7.3000e-02, -2.7051e-01,  6.9139e-01, -9.5220e-02,  1.6011e-01,\n",
       "          -1.2424e-01, -1.8378e-01, -1.4660e-01,  6.3168e-02, -1.4993e-01,\n",
       "          -5.7103e-01,  2.2359e-01,  1.0795e-01,  1.0616e-01,  2.4938e-01,\n",
       "           3.0051e-01, -5.3310e-01,  3.5771e-01, -1.8586e-01,  1.9984e-01,\n",
       "          -1.7034e-01,  3.9097e-01,  7.2686e-02,  6.6708e-02,  6.4364e-02,\n",
       "          -2.7825e-03,  4.1006e-02,  1.6895e-01,  1.6615e-01,  1.5083e-01,\n",
       "          -2.4489e-01,  2.5287e-01,  2.8236e-01, -1.2226e-01, -6.2271e-02,\n",
       "          -3.2344e-01, -1.4886e-01,  2.7082e-01,  2.2715e-01,  8.1514e-02,\n",
       "           1.3073e-01,  3.6592e-02,  1.9625e-01, -4.5066e-01, -8.4207e-02,\n",
       "          -4.4004e-02,  6.8498e-01, -4.3684e-01,  3.2238e-01, -2.3073e-01,\n",
       "           7.5247e-01, -6.6770e-02,  2.2413e-01, -1.4485e-02, -3.3402e-01,\n",
       "           2.1579e-01,  1.1589e-01, -2.0035e-02,  7.2176e-02,  7.8595e-03,\n",
       "          -3.8296e-01, -3.3838e-01, -1.3486e-01, -6.1989e-03, -6.2019e-02,\n",
       "           6.2820e-01, -2.1117e-01, -2.6801e-01, -6.1224e-02, -1.1764e-02,\n",
       "          -4.3967e-01, -2.3267e-01, -1.8804e-01, -3.7182e-02,  3.8333e-01,\n",
       "          -1.3781e-01, -3.5373e-01, -2.2351e-01, -8.2712e-02, -1.5721e-01,\n",
       "           9.4948e-02,  1.4660e-01,  3.1672e-01, -4.0633e-02,  1.6952e-01,\n",
       "          -7.6335e-03, -2.5042e-01, -3.8951e-01,  1.8332e-01, -9.5388e-02,\n",
       "           1.1239e-02,  1.9264e-02,  3.2269e-01, -8.6549e-02,  3.8693e-01,\n",
       "          -1.6739e-01, -3.0298e-02, -1.7119e-01,  1.9985e-01,  2.5704e-01,\n",
       "          -5.1953e-02, -2.4095e-01,  1.3730e-01,  9.3880e-02,  7.3841e-02,\n",
       "          -1.2848e-01,  2.9057e-01, -1.1148e-01],\n",
       "         [ 5.8739e-02,  4.4403e-01,  7.4717e-03,  7.7779e-01, -9.2546e-02,\n",
       "           3.4506e-01, -4.5087e-02,  3.1790e-01, -1.3889e-01,  5.7013e-01,\n",
       "           1.0779e-01, -2.1758e-01, -3.3891e-01, -8.6099e-02, -2.8149e-01,\n",
       "           3.4211e-01,  8.1234e-01,  2.3516e-01, -1.9481e-01, -1.0353e-01,\n",
       "          -1.7046e-01, -4.1368e-01,  8.7358e-01, -8.0044e-02,  5.4583e-02,\n",
       "          -8.9880e-02, -2.1214e-01,  4.5729e-02, -3.2929e-03, -4.6835e-01,\n",
       "          -7.7877e-01,  5.0106e-01,  6.4574e-02,  3.4545e-01,  5.8667e-01,\n",
       "           3.8755e-01, -2.0034e-01,  5.5947e-01, -1.9067e-01,  7.3065e-02,\n",
       "          -4.3936e-01,  4.0604e-01, -1.7183e-01, -1.3500e-01,  1.7931e-01,\n",
       "           4.2362e-01,  6.5490e-02,  1.7386e-01, -1.7314e-01, -2.0886e-01,\n",
       "          -3.9623e-01,  1.1021e-01,  2.4500e-01, -3.9526e-02, -5.6648e-02,\n",
       "          -3.2220e-01, -2.3910e-01,  6.4216e-01,  2.9037e-01,  8.0930e-02,\n",
       "          -9.7782e-02, -7.4459e-03, -4.2255e-02, -3.1389e-01, -2.0526e-01,\n",
       "          -7.4457e-03,  7.0111e-01, -6.4289e-01, -1.0152e-03,  3.0793e-02,\n",
       "           5.3432e-01,  1.4218e-03,  9.9515e-02, -4.0426e-01, -4.6667e-01,\n",
       "           5.6351e-02,  9.7509e-02,  2.2858e-02,  3.1652e-02,  1.9854e-01,\n",
       "          -4.9031e-01, -3.1357e-01, -1.9957e-01, -3.0155e-02, -2.4221e-01,\n",
       "           5.0708e-01, -1.1487e-01, -3.8569e-01,  2.1764e-01, -3.6582e-06,\n",
       "          -3.5100e-01, -3.7493e-01, -3.6831e-01,  1.2101e-01,  1.2304e-01,\n",
       "           4.0131e-02, -1.9245e-01, -1.2619e-01,  7.3046e-02, -1.8295e-01,\n",
       "           3.1142e-02,  1.8781e-01,  4.5967e-01, -2.5148e-01,  2.2772e-01,\n",
       "           1.3897e-02, -3.7976e-01, -3.3415e-01,  2.0583e-01, -2.9484e-01,\n",
       "          -1.5435e-02,  2.7907e-01,  1.3125e-01,  1.7672e-01,  2.6661e-01,\n",
       "          -2.9558e-01,  2.6974e-01, -3.5024e-02, -6.1463e-02,  9.5610e-03,\n",
       "          -1.0858e-01, -1.8529e-01, -5.5280e-02,  1.5041e-01,  1.3732e-01,\n",
       "          -2.3697e-02,  4.9879e-01, -2.8159e-01]],\n",
       "\n",
       "        [[-3.2158e-02,  5.3342e-02,  3.3757e-01, -1.0886e-01,  4.6152e-01,\n",
       "          -4.4528e-01,  1.1053e-01,  1.8012e-01, -1.6017e-01, -3.5121e-01,\n",
       "          -4.0510e-01, -2.5272e-01,  7.6824e-02,  1.0088e-02,  1.7211e-01,\n",
       "           3.1816e-01,  7.6054e-02, -1.9763e-01, -1.8514e-01, -3.2075e-01,\n",
       "          -2.3741e-01,  2.0813e-01,  2.3065e-01,  3.0699e-01, -1.0296e-01,\n",
       "          -6.5506e-01,  9.8679e-02, -5.7191e-01,  2.1980e-02,  1.3093e-01,\n",
       "           2.9620e-02, -1.7060e-01, -2.7300e-01, -2.4887e-01, -1.1788e-01,\n",
       "          -1.1408e-01,  2.3386e-01, -1.7605e-01,  2.4083e-01,  7.3817e-02,\n",
       "           3.9570e-01, -6.7829e-02,  1.0410e-01, -2.1890e-01, -1.9735e-01,\n",
       "          -3.0601e-02, -3.7762e-01, -4.8520e-01, -1.8931e-02,  2.0442e-01,\n",
       "          -5.4375e-01,  3.9623e-01, -2.0238e-01,  4.6011e-01, -7.4094e-02,\n",
       "           1.0403e-01,  3.2369e-03, -1.3107e-01, -9.6386e-02, -5.2659e-01,\n",
       "          -2.1275e-01,  3.1088e-01,  1.8700e-01,  2.6456e-01, -4.0777e-02,\n",
       "          -3.6780e-01,  4.9674e-01, -3.3581e-01, -4.3099e-01,  3.8174e-02,\n",
       "          -1.1930e-01, -1.9929e-01,  1.0481e-01, -9.5689e-02,  5.6437e-01,\n",
       "          -3.4928e-01,  3.1581e-01, -5.5302e-02,  9.1058e-02,  9.4095e-02,\n",
       "           3.0219e-01, -3.3968e-01, -2.2246e-01,  3.8977e-01, -2.1514e-03,\n",
       "          -4.2822e-02,  3.8363e-01, -3.4765e-02, -2.4101e-02, -4.0338e-01,\n",
       "           2.6455e-02,  1.9463e-01,  6.2997e-01,  1.5745e-01, -7.1839e-02,\n",
       "           1.3098e-03, -1.6048e-01,  3.8371e-01,  3.5982e-01,  1.3624e-01,\n",
       "          -4.4711e-01, -3.4043e-01, -7.8280e-02,  6.2723e-04,  2.5435e-01,\n",
       "          -1.6605e-01,  3.2512e-01, -4.0280e-01, -2.5502e-01, -4.0743e-01,\n",
       "           5.0016e-01,  4.4228e-02,  2.4707e-03,  3.1044e-01, -1.7441e-01,\n",
       "          -3.0297e-01,  7.7303e-02,  2.0538e-01, -2.8800e-01, -2.6675e-01,\n",
       "           2.0498e-01, -7.6341e-01, -2.8476e-01,  1.7875e-01,  3.5906e-01,\n",
       "          -8.1857e-02, -2.2884e-01, -4.5272e-01],\n",
       "         [-8.4916e-02, -5.1402e-03,  3.0647e-01, -1.7597e-01,  4.4387e-01,\n",
       "          -4.6684e-01,  1.7837e-01,  3.0653e-01, -1.7273e-01, -1.6241e-01,\n",
       "          -5.3264e-01, -2.1384e-01,  7.7421e-02,  7.8921e-02,  1.7963e-01,\n",
       "           1.2800e-01, -5.9171e-02, -1.2456e-01, -1.4191e-01, -3.6205e-01,\n",
       "          -2.4398e-01,  1.9808e-01,  2.2601e-01,  2.8961e-01, -1.2072e-01,\n",
       "          -7.1052e-01,  1.4140e-02, -4.3821e-01,  9.0920e-02,  1.6373e-01,\n",
       "           2.4065e-01, -3.7050e-01, -1.5176e-01, -3.1279e-01, -6.7333e-02,\n",
       "          -2.2002e-01,  5.7986e-02, -1.3184e-01,  1.9872e-01, -7.6713e-03,\n",
       "           3.7800e-01, -3.0750e-01,  1.0442e-01, -2.3385e-01, -2.1961e-01,\n",
       "          -3.4642e-01, -1.9986e-01, -3.5400e-01,  3.6411e-03,  1.7997e-01,\n",
       "          -4.8342e-01,  2.0727e-01, -2.5855e-01,  4.2737e-01, -1.4736e-01,\n",
       "           1.4780e-02, -1.2992e-01, -9.1708e-02, -4.7622e-02, -5.6532e-01,\n",
       "          -6.9932e-02,  3.1022e-01,  2.0157e-01,  8.5479e-02,  3.8251e-02,\n",
       "          -3.2054e-01,  5.1410e-01, -3.5548e-01, -3.3085e-01,  6.1426e-02,\n",
       "          -2.3616e-01, -2.5767e-01,  1.1161e-01,  8.6468e-02,  5.9757e-01,\n",
       "          -1.6055e-01,  4.5418e-01, -8.3025e-02,  2.8931e-01,  1.7611e-01,\n",
       "           4.2669e-01, -2.9120e-01, -3.4363e-01,  4.5826e-01, -1.8337e-01,\n",
       "          -4.8175e-02,  2.0415e-01, -6.6065e-02,  1.1059e-01, -3.7342e-01,\n",
       "           1.2567e-02,  1.8135e-01,  5.5806e-01,  8.7888e-02, -1.3389e-01,\n",
       "          -9.7213e-02, -3.8798e-02,  4.0540e-01,  3.9395e-01,  2.4241e-01,\n",
       "          -5.2602e-01, -2.3047e-01, -1.8697e-01, -7.7760e-02,  2.0741e-01,\n",
       "          -2.1305e-01,  2.8676e-01, -2.6893e-01, -2.7075e-01, -2.2435e-01,\n",
       "           6.4146e-01,  2.4797e-02,  1.4464e-01,  4.7661e-01, -1.1406e-01,\n",
       "          -3.4679e-01,  2.5778e-01,  1.6719e-01, -2.8082e-01, -1.5407e-01,\n",
       "           1.8389e-01, -7.5492e-01, -3.0940e-01,  5.8239e-02,  4.3803e-01,\n",
       "          -2.4301e-01, -1.7270e-01, -3.6690e-01]],\n",
       "\n",
       "        [[ 6.2878e-01, -1.7645e-01, -1.3399e-01,  4.8822e-01,  2.4397e-02,\n",
       "          -2.2302e-01, -1.8799e-02, -1.2134e+00,  1.7881e-01,  1.2124e-01,\n",
       "          -1.3678e-02, -2.3542e-01,  1.0178e-01, -4.4586e-01,  2.1979e-01,\n",
       "           6.5119e-01, -4.1986e-01, -8.7434e-02, -3.5701e-01,  2.4648e-01,\n",
       "           2.2700e-01, -2.0367e-01,  6.9290e-01,  3.9062e-01, -9.0793e-02,\n",
       "           2.2016e-01, -1.3462e-01, -5.6804e-01,  3.9229e-01, -3.6976e-01,\n",
       "          -3.2557e-01, -1.0618e-01, -7.3118e-01,  1.3660e-01,  3.4113e-01,\n",
       "           2.8701e-01, -1.0008e-01,  9.6357e-02, -4.4128e-01, -1.7621e-01,\n",
       "          -7.1509e-01,  5.1446e-01,  2.2860e-01, -3.1776e-01,  3.5277e-02,\n",
       "           5.6873e-01,  9.4597e-01, -5.0992e-01,  1.4606e-01,  1.0216e-02,\n",
       "           1.7201e-01,  1.3751e-01,  1.7509e-01,  3.6573e-02,  3.5046e-01,\n",
       "          -3.0939e-01, -4.4141e-01,  2.5624e-01, -4.2720e-01,  7.2222e-02,\n",
       "          -3.0218e-01, -7.0468e-01, -1.0906e-01,  3.2946e-01, -1.9206e-01,\n",
       "          -2.2061e-02, -8.2522e-01,  3.0516e-01, -2.3080e-01, -1.7819e-01,\n",
       "          -3.1659e-02, -4.9570e-01, -2.5348e-01,  3.8705e-01, -2.4720e-01,\n",
       "           3.0086e-01, -3.4645e-01, -2.0635e-01, -6.2303e-01, -2.8304e-01,\n",
       "          -3.9737e-01,  5.8894e-02,  7.7560e-02,  7.6097e-02, -6.2173e-01,\n",
       "           6.3445e-01, -4.1859e-01, -3.6897e-01, -9.2080e-02,  4.2175e-01,\n",
       "           1.3007e-01,  2.1707e-01, -5.5789e-01, -3.5211e-01,  9.5685e-01,\n",
       "           3.4335e-01, -2.0109e-01,  2.5198e-01, -3.8020e-01, -4.5306e-01,\n",
       "           2.9462e-01,  5.0464e-01,  1.7126e-01,  3.9903e-01,  3.3076e-01,\n",
       "           1.3329e-01, -2.1126e-01,  1.8941e-01,  2.8987e-02, -1.9233e-01,\n",
       "          -8.6994e-03, -3.3240e-02,  1.2585e-01,  6.3116e-02, -3.1951e-01,\n",
       "           3.2679e-01,  5.4644e-02, -6.1949e-02, -8.3747e-03, -1.7514e-01,\n",
       "          -2.2236e-01, -9.3153e-02,  4.2104e-01, -3.5977e-01, -2.9416e-01,\n",
       "           3.1958e-01,  1.7092e-01,  4.4551e-01],\n",
       "         [ 7.3078e-01, -1.3965e-01,  2.3458e-02,  5.4479e-01,  2.0459e-01,\n",
       "          -3.8562e-01, -1.1900e-01, -9.5638e-01,  1.9061e-01,  1.6220e-01,\n",
       "           2.4352e-01, -5.2133e-01,  2.0457e-01, -4.6958e-01,  4.4375e-01,\n",
       "           4.3399e-01, -3.2648e-01,  4.5875e-02, -6.5530e-02,  7.4142e-02,\n",
       "           4.0850e-01, -1.5942e-01,  1.1138e+00, -3.0377e-02,  1.0019e-01,\n",
       "           1.3403e-01,  2.7916e-01, -3.2694e-01,  9.1918e-02, -4.4682e-01,\n",
       "          -3.0834e-01,  1.5057e-01, -5.5916e-01,  2.2179e-02,  1.8607e-01,\n",
       "           5.3124e-01, -4.2866e-02, -1.7105e-01, -4.2012e-01,  1.8909e-01,\n",
       "          -1.0398e+00,  1.0818e+00,  1.5610e-01, -3.5268e-01,  4.4059e-01,\n",
       "           7.3700e-01,  1.0171e+00, -4.0545e-01,  3.3974e-01,  7.6910e-02,\n",
       "           4.0302e-01,  1.0871e-01,  3.8032e-01,  4.1934e-02,  5.7279e-01,\n",
       "          -1.2307e-01, -5.1995e-01,  3.1104e-01, -6.3910e-01,  2.6566e-01,\n",
       "          -4.1301e-01, -2.8671e-01, -3.1840e-02,  1.9185e-01, -2.2602e-01,\n",
       "          -9.2306e-02, -6.3111e-01,  5.6346e-01, -2.5854e-01, -3.5932e-01,\n",
       "           6.2063e-02, -3.3385e-01, -4.0341e-01,  2.3453e-01,  4.0951e-02,\n",
       "           1.9548e-02, -1.7120e-01, -1.3540e-01, -6.2611e-01,  3.3825e-03,\n",
       "          -3.9145e-01, -1.2212e-01,  2.1096e-01, -8.7916e-02, -5.8959e-01,\n",
       "           7.4630e-01, -7.2346e-01, -7.1253e-02, -6.3549e-02,  6.6183e-01,\n",
       "           2.2258e-01,  1.3404e-01, -7.7652e-01, -8.4906e-01,  1.1263e+00,\n",
       "           5.2865e-01, -2.7729e-01,  4.7264e-01, -3.5919e-01, -1.3340e-01,\n",
       "           2.9188e-01,  7.0473e-01,  2.1095e-01,  1.1215e-01,  2.6166e-01,\n",
       "          -5.3542e-03, -3.1314e-01,  2.7104e-01,  1.4602e-01, -1.6723e-01,\n",
       "          -2.5469e-01, -3.0203e-01, -1.5123e-01,  1.2356e-01, -1.3198e-01,\n",
       "           3.0190e-01,  2.6734e-02,  2.4266e-01,  2.5211e-02, -3.6783e-01,\n",
       "          -3.9604e-01,  4.8571e-01,  3.5941e-01, -3.3938e-01, -2.0347e-01,\n",
       "           3.9296e-01,  5.6424e-02,  6.1698e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, head_num, attention_dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        # head number * head_dim = hidden_dim\n",
    "        self.head_dim = hidden_dim // head_num\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, x, attention_mask = None):\n",
    "        # x shape: [batch_size, seq_len, hidden_dim]\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Q, K, V shape: [batch_size, seq_len, hidden_dim]\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "\n",
    "        # [batch_size, seq_len, hidden_dim] -> [batch_size, head_num, seq_len, head_dim]\n",
    "        # hidden_dim -> head_num * head_dim\n",
    "        Q_state = Q.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        K_state = K.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        V_state = V.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attention_weight = torch.matmul(\n",
    "            # [batch_size, head_num, seq_len, head_dim] \n",
    "            #           -> [batch_size, head_num, head_dim, seq_len]\n",
    "            Q_state, K_state.transpose(-1, -2) \n",
    "        ) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0, \n",
    "                float(\"-inf\")\n",
    "            )\n",
    "        \n",
    "        # print(attention_weight)\n",
    "\n",
    "        attention_weight = torch.softmax(attention_weight, dim = -1)\n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "\n",
    "        # [b, head_num, seq_len, head_dim]\n",
    "        output_mid = torch.matmul(attention_weight, V_state)\n",
    "\n",
    "        # [b, head_num, seq_len, head_dim] -> [b, seq_len, hidden_dim]\n",
    "        output_mid = output_mid.transpose(1, 2).contiguous()\n",
    "        output_mid = output_mid.view(batch_size, seq_len, -1)\n",
    "\n",
    "        output = self.out_proj(output_mid)\n",
    "        return output\n",
    "\n",
    "attention_mask = (\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [0, 1],\n",
    "            [1, 1],\n",
    "            [1, 0]\n",
    "        ]\n",
    "    ).unsqueeze(1).unsqueeze(2).expand(3, 8, 2, 2)\n",
    ")\n",
    "\n",
    "x = torch.randn(3, 2, 128)\n",
    "\n",
    "# head_dim = 16\n",
    "net = MultiHeadAttention(128, 8)\n",
    "net(x, attention_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
